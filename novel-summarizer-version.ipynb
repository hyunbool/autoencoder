{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer \n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import urllib.request\n",
    "import tensorflow as tf\n",
    "np.random.seed(seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "os.environ[\"SM_FRAMEWORK\"] = \"tf.keras\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "books_path = \"./books-autoencoder/\"\n",
    "new_path = \"../\"\n",
    "books_list = sorted(os.listdir(books_path))\n",
    "\n",
    "documents = list()\n",
    "\n",
    "for i in books_list:\n",
    "    f = open(books_path + i, 'r')\n",
    "    doc = list(map(lambda x: x.replace(\"\\n\", \"\"), f.readlines()))\n",
    "    doc = [x.replace(\"</s>\", \" </s>\")[:-5] for x in doc]\n",
    "    documents.append(doc)\n",
    "\n",
    "\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_total = list()\n",
    "for doc in documents:\n",
    "    doc_total.extend(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_len = [len(s.split()) for s in doc_total]\n",
    "seg_len = [len(s) for s in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(text_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(seg_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "텍스트의 최소 길이 : 3\n",
      "텍스트의 최대 길이 : 2686\n",
      "텍스트의 평균 길이 : 83.44231867504283\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('텍스트의 최소 길이 : {}'.format(np.min(text_len)))\n",
    "print('텍스트의 최대 길이 : {}'.format(np.max(text_len)))\n",
    "print('텍스트의 평균 길이 : {}'.format(np.mean(text_len)))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.boxplot(text_len)\n",
    "plt.title('Text')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.title('Text')\n",
    "plt.hist(text_len, bins=40)\n",
    "plt.xlabel('length of samples')\n",
    "plt.ylabel('number of samples')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_max_len = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "텍스트의 최소 길이 : 29\n",
      "텍스트의 최대 길이 : 98\n",
      "텍스트의 평균 길이 : 77.82222222222222\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('텍스트의 최소 길이 : {}'.format(np.min(seg_len)))\n",
    "print('텍스트의 최대 길이 : {}'.format(np.max(seg_len)))\n",
    "print('텍스트의 평균 길이 : {}'.format(np.mean(seg_len)))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.boxplot(seg_len)\n",
    "plt.title('Text')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.title('Text')\n",
    "plt.hist(seg_len, bins=40)\n",
    "plt.xlabel('length of samples')\n",
    "plt.ylabel('number of samples')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_max_len = 90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input = documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_input = list()\n",
    "for doc in encoder_input:\n",
    "    tmp = list()\n",
    "    for x in doc:\n",
    "        x = 'sostoken ' + x\n",
    "        tmp.append(x)\n",
    "    decoder_input.append(tmp)\n",
    "    \n",
    "decoder_target = list()\n",
    "for doc in encoder_input:\n",
    "    tmp = list()\n",
    "    for x in doc:\n",
    "        x = x + ' eostoken'\n",
    "        tmp.append(x)\n",
    "    decoder_target.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tako/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/home/tako/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  \n",
      "/home/tako/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "encoder_input = np.array(encoder_input)\n",
    "decoder_input = np.array(decoder_input)\n",
    "decoder_target = np.array(decoder_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 45  98  54 107 128 124 123  37  97  88 118  87  57  59  90 121  72  38\n",
      "  31 104   9  50  94   3  60  77 100  61   6   1  23  86 132  95  53  27\n",
      "  10  26  65  52  74 102  15  47  79   5  89  92   7  68 117 109   4 105\n",
      "  20  62  96  14  42 106 131  76  82  11  64  55 130  13  19  32  56   8\n",
      "  51 119  78  24 122  16  63 133  58  81  28 108  73  21  46  41  49  83\n",
      "  43 111  71  70 110 115  22  75  99  39  85 125  66  44  12 101  69   2\n",
      "  84  93  17  18  33 113  29  30 114 127 103  67  25  48 134 116  40  91\n",
      " 120  35 112 129  80 126  34   0  36]\n"
     ]
    }
   ],
   "source": [
    "indices = np.arange(encoder_input.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "print(indices)\n",
    "\n",
    "encoder_input = encoder_input[indices]\n",
    "decoder_input = decoder_input[indices]\n",
    "decoder_target = decoder_target[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "테스트 데이터의 수 : 27\n"
     ]
    }
   ],
   "source": [
    "n_of_val = int(len(encoder_input)*0.2)\n",
    "print('테스트 데이터의 수 :',n_of_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_train = encoder_input[:-n_of_val]\n",
    "decoder_input_train = decoder_input[:-n_of_val]\n",
    "decoder_target_train = decoder_target[:-n_of_val]\n",
    "\n",
    "encoder_input_test = encoder_input[-n_of_val:]\n",
    "decoder_input_test = decoder_input[-n_of_val:]\n",
    "decoder_target_test = decoder_target[-n_of_val:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108\n",
      "108\n",
      "108\n"
     ]
    }
   ],
   "source": [
    "print(len(encoder_input_train))\n",
    "print(len(decoder_input_train))\n",
    "print(len(decoder_target_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 데이터의 개수 : 108\n",
      "훈련 레이블의 개수 : 108\n",
      "테스트 데이터의 개수 : 27\n",
      "테스트 레이블의 개수 : 27\n"
     ]
    }
   ],
   "source": [
    "print('훈련 데이터의 개수 :', len(encoder_input_train))\n",
    "print('훈련 레이블의 개수 :',len(decoder_input_train))\n",
    "print('테스트 데이터의 개수 :',len(encoder_input_test))\n",
    "print('테스트 레이블의 개수 :',len(decoder_input_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_total = list()\n",
    "for doc in encoder_input_train:\n",
    "    encoder_total.extend(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_tokenizer = Tokenizer()\n",
    "src_tokenizer.fit_on_texts(encoder_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합(vocabulary)의 크기 : 129279\n",
      "등장 빈도가 6번 이하인 희귀 단어의 수: 118799\n",
      "단어 집합에서 희귀 단어를 제외시킬 경우의 단어 집합의 크기 10480\n",
      "단어 집합에서 희귀 단어의 비율: 91.89350165146698\n",
      "전체 등장 빈도에서 희귀 단어 등장 빈도 비율: 29.242224012892827\n"
     ]
    }
   ],
   "source": [
    "threshold = 7\n",
    "total_cnt = len(src_tokenizer.word_index) # 단어의 수\n",
    "rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n",
    "total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n",
    "rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\n",
    "\n",
    "# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\n",
    "for key, value in src_tokenizer.word_counts.items():\n",
    "    total_freq = total_freq + value\n",
    "\n",
    "    # 단어의 등장 빈도수가 threshold보다 작으면\n",
    "    if(value < threshold):\n",
    "        rare_cnt = rare_cnt + 1\n",
    "        rare_freq = rare_freq + value\n",
    "\n",
    "print('단어 집합(vocabulary)의 크기 :',total_cnt)\n",
    "print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))\n",
    "print('단어 집합에서 희귀 단어를 제외시킬 경우의 단어 집합의 크기 %s'%(total_cnt - rare_cnt))\n",
    "print(\"단어 집합에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\n",
    "print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_vocab = 110000\n",
    "src_tokenizer = Tokenizer(num_words = src_vocab) \n",
    "src_tokenizer.fit_on_texts(encoder_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeded_encoder_input_train = list()\n",
    "for doc in encoder_input_train:\n",
    "    doc = src_tokenizer.texts_to_sequences(doc)\n",
    "    embeded_encoder_input_train.append(doc)\n",
    "encoder_input_train = embeded_encoder_input_train\n",
    "\n",
    "embeded_encoder_input_test = list()\n",
    "for doc in encoder_input_test:\n",
    "    doc = src_tokenizer.texts_to_sequences(doc)\n",
    "    embeded_encoder_input_test.append(doc)\n",
    "encoder_input_test = embeded_encoder_input_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_total = list()\n",
    "for doc in decoder_input_train:\n",
    "    decoder_total.extend(doc)\n",
    "for doc in decoder_target_train:\n",
    "    decoder_total.extend(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "tar_tokenizer = Tokenizer()\n",
    "tar_tokenizer.fit_on_texts(decoder_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합(vocabulary)의 크기 : 129281\n",
      "등장 빈도가 5번 이하인 희귀 단어의 수: 103224\n",
      "단어 집합에서 희귀 단어를 제외시킬 경우의 단어 집합의 크기 26057\n",
      "단어 집합에서 희귀 단어의 비율: 79.84467941924954\n",
      "전체 등장 빈도에서 희귀 단어 등장 빈도 비율: 19.06777572298822\n"
     ]
    }
   ],
   "source": [
    "threshold = 6\n",
    "total_cnt = len(tar_tokenizer.word_index) # 단어의 수\n",
    "rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n",
    "total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n",
    "rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\n",
    "\n",
    "# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\n",
    "for key, value in tar_tokenizer.word_counts.items():\n",
    "    total_freq = total_freq + value\n",
    "\n",
    "    # 단어의 등장 빈도수가 threshold보다 작으면\n",
    "    if(value < threshold):\n",
    "        rare_cnt = rare_cnt + 1\n",
    "        rare_freq = rare_freq + value\n",
    "\n",
    "print('단어 집합(vocabulary)의 크기 :',total_cnt)\n",
    "print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))\n",
    "print('단어 집합에서 희귀 단어를 제외시킬 경우의 단어 집합의 크기 %s'%(total_cnt - rare_cnt))\n",
    "print(\"단어 집합에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\n",
    "print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "tar_vocab = 110000\n",
    "tar_tokenizer = Tokenizer(num_words = tar_vocab) \n",
    "tar_tokenizer.fit_on_texts(decoder_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeded_decoder_input_train = list()\n",
    "for doc in decoder_input_train:\n",
    "    doc = src_tokenizer.texts_to_sequences(doc)\n",
    "    embeded_decoder_input_train.append(doc)\n",
    "decoder_input_train = embeded_decoder_input_train\n",
    "\n",
    "embeded_decoder_input_test = list()\n",
    "for doc in decoder_input_test:\n",
    "    doc = src_tokenizer.texts_to_sequences(doc)\n",
    "    embeded_decoder_input_test.append(doc)\n",
    "decoder_input_test = embeded_decoder_input_test\n",
    "\n",
    "embeded_decoder_target_train = list()\n",
    "for doc in decoder_target_train:\n",
    "    doc = src_tokenizer.texts_to_sequences(doc)\n",
    "    embeded_decoder_target_train.append(doc)\n",
    "decoder_target_train = embeded_decoder_target_train\n",
    "\n",
    "embeded_decoder_target_test = list()\n",
    "for doc in decoder_target_test:\n",
    "    doc = src_tokenizer.texts_to_sequences(doc)\n",
    "    embeded_decoder_target_test.append(doc)\n",
    "decoder_target_test = embeded_decoder_target_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_train = [index for index, sentence in enumerate(doc) for doc in decoder_input_train if len(sentence) == 1]\n",
    "drop_test = [index for index, sentence in enumerate(doc) for doc in decoder_input_test if len(sentence) == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 데이터의 개수 : 108\n",
      "훈련 레이블의 개수 : 108\n",
      "테스트 데이터의 개수 : 27\n",
      "테스트 레이블의 개수 : 27\n"
     ]
    }
   ],
   "source": [
    "encoder_input_train = np.delete(encoder_input_train, drop_train, axis=0)\n",
    "decoder_input_train = np.delete(decoder_input_train, drop_train, axis=0)\n",
    "decoder_target_train = np.delete(decoder_target_train, drop_train, axis=0)\n",
    "\n",
    "encoder_input_test = np.delete(encoder_input_test, drop_test, axis=0)\n",
    "decoder_input_test = np.delete(decoder_input_test, drop_test, axis=0)\n",
    "decoder_target_test = np.delete(decoder_target_test, drop_test, axis=0)\n",
    "\n",
    "print('훈련 데이터의 개수 :', len(encoder_input_train))\n",
    "print('훈련 레이블의 개수 :',len(decoder_input_train))\n",
    "print('테스트 데이터의 개수 :',len(encoder_input_test))\n",
    "print('테스트 레이블의 개수 :',len(decoder_input_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_sent_encoder_input_train = list()\n",
    "for doc in encoder_input_train:\n",
    "    doc = pad_sequences(doc, maxlen = text_max_len, padding='post')\n",
    "    padded_sent_encoder_input_train.append(doc)\n",
    "encoder_input_train = padded_sent_encoder_input_train\n",
    "\n",
    "padded_sent_encoder_input_test = list()\n",
    "for doc in encoder_input_test:\n",
    "    doc = pad_sequences(doc, maxlen = text_max_len, padding='post')\n",
    "    padded_sent_encoder_input_test.append(doc)\n",
    "encoder_input_test = padded_sent_encoder_input_test\n",
    "\n",
    "padded_sent_decoder_input_train = list()\n",
    "for doc in decoder_input_train:\n",
    "    doc = pad_sequences(doc, maxlen = text_max_len, padding='post')\n",
    "    padded_sent_decoder_input_train.append(doc)\n",
    "decoder_input_train = padded_sent_decoder_input_train\n",
    "\n",
    "padded_sent_decoder_target_train = list()\n",
    "for doc in decoder_target_train:\n",
    "    doc = pad_sequences(doc, maxlen = text_max_len, padding='post')\n",
    "    padded_sent_decoder_target_train.append(doc)\n",
    "decoder_target_train = padded_sent_decoder_target_train\n",
    "\n",
    "padded_sent_decoder_input_test = list()\n",
    "for doc in decoder_input_test:\n",
    "    doc = pad_sequences(doc, maxlen = text_max_len, padding='post')\n",
    "    padded_sent_decoder_input_test.append(doc)\n",
    "decoder_input_test = padded_sent_decoder_input_test\n",
    "\n",
    "padded_sent_decoder_target_test = list()\n",
    "for doc in decoder_target_test:\n",
    "    doc = pad_sequences(doc, maxlen = text_max_len, padding='post')\n",
    "    padded_sent_decoder_target_test.append(doc)\n",
    "decoder_target_test = padded_sent_decoder_target_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_train = pad_sequences(encoder_input_train, maxlen = seg_max_len, padding='post')\n",
    "encoder_input_test = pad_sequences(encoder_input_test, maxlen = seg_max_len, padding='post')\n",
    "decoder_input_train = pad_sequences(decoder_input_train, maxlen = seg_max_len, padding='post')\n",
    "decoder_target_train = pad_sequences(decoder_target_train, maxlen = seg_max_len, padding='post')\n",
    "decoder_input_test = pad_sequences(decoder_input_test, maxlen = seg_max_len, padding='post')\n",
    "decoder_target_test = pad_sequences(decoder_target_test, maxlen = seg_max_len, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 128\n",
    "hidden_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/thushv89/attention_keras/master/src/layers/attention.py\", filename=\"attention.py\")\n",
    "from attention import AttentionLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(90, 150)"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# seg_max_len = 90\n",
    "# text_max_len = 150\n",
    "encoder_input_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[26056,    41, 26057, ...,     0,     0,     0],\n",
       "       [  306, 19060,   293, ...,   756,  8111,    23],\n",
       "       [44366, 44367, 44368, ...,     0,     0,     0],\n",
       "       ...,\n",
       "       [ 2183, 16020,  7722, ...,     0,     0,     0],\n",
       "       [27849, 37740,  1949, ...,     0,     0,     0],\n",
       "       [ 6470,  4470,     1, ...,     0,     0,     0]], dtype=int32)"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_input_train[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autoencoder(hidden_size, text_max_len, src_vocab, tar_vocab, embedding_dim, name='autoencoder'):\n",
    "    encoder_inputs = Input(shape=(seg_max_len, text_max_len, ))\n",
    "    decoder_inputs = Input(shape=(None,))\n",
    "\n",
    "    for t in range(encoder_inputs.shape[1]):\n",
    "        # 인코더의 임베딩 층\n",
    "        enc_emb = Embedding(src_vocab, embedding_dim)(encoder_inputs[:, t])\n",
    "\n",
    "        # 인코더의 LSTM 1\n",
    "        encoder_lstm1 = LSTM(hidden_size, return_sequences=True, return_state=True ,dropout = 0.4, recurrent_dropout = 0.4)\n",
    "        encoder_output1, state_h1, state_c1 = encoder_lstm1(enc_emb)\n",
    "\n",
    "        # 인코더의 LSTM 2\n",
    "        encoder_lstm2 = LSTM(hidden_size, return_sequences=True, return_state=True, dropout=0.4, recurrent_dropout=0.4)\n",
    "        encoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1)\n",
    "\n",
    "        # 인코더의 LSTM 3\n",
    "        encoder_lstm3 = LSTM(hidden_size, return_state=True, return_sequences=True, dropout=0.4, recurrent_dropout=0.4)\n",
    "        encoder_outputs, state_h, state_c= encoder_lstm3(encoder_output2)\n",
    "    \n",
    "    \n",
    "    # 디코더의 임베딩 층\n",
    "    dec_emb_layer = Embedding(tar_vocab, embedding_dim)\n",
    "    dec_emb = dec_emb_layer(decoder_inputs[:, :, 0]) # 일단 책의 첫 문장 넣기 \n",
    "\n",
    "    # 디코더의 LSTM\n",
    "    decoder_lstm = LSTM(hidden_size, return_sequences = True, return_state = True, dropout = 0.4, recurrent_dropout=0.2)\n",
    "    decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state = [state_h, state_c])\n",
    "    \n",
    "    attn_layer = AttentionLayer(name='attention_layer')\n",
    "    attn_out, attn_states = attn_layer([encoder_outputs, decoder_outputs])\n",
    "\n",
    "    # 어텐션의 결과와 디코더의 hidden state들을 연결\n",
    "    decoder_concat_input = Concatenate(axis = -1, name='concat_layer')([decoder_outputs, attn_out])\n",
    "\n",
    "    # 디코더의 출력층\n",
    "    decoder_softmax_layer = Dense(tar_vocab, activation='softmax')\n",
    "    decoder_softmax_outputs = decoder_softmax_layer(decoder_concat_input) \n",
    "\n",
    "    #return Model(inputs = [encoder_outputs, state_h, state_c, decoder_inputs], outputs = decoder_softmax_outputs)\n",
    "    \n",
    "    return Model([encoder_inputs, decoder_inputs], outputs = decoder_softmax_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Index out of range using input dim 2; input has only 2 dims for 'strided_slice_452' (op: 'StridedSlice') with input shapes: [?,?], [3], [3], [3] and with computed input tensors: input[3] = <1 1 1>.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[1;32m   1609\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1610\u001b[0;31m     \u001b[0mc_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_FinishOperation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_desc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1611\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Index out of range using input dim 2; input has only 2 dims for 'strided_slice_452' (op: 'StridedSlice') with input shapes: [?,?], [3], [3], [3] and with computed input tensors: input[3] = <1 1 1>.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-265-e9f19f3a5a08>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mautoencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_max_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtar_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'autoencoder'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-264-63bf45c8b44d>\u001b[0m in \u001b[0;36mautoencoder\u001b[0;34m(hidden_size, text_max_len, src_vocab, tar_vocab, embedding_dim, name)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;31m# 디코더의 임베딩 층\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mdec_emb_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtar_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mdec_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdec_emb_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_inputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 일단 책의 첫 문장 넣기\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;31m# 디코더의 LSTM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/array_ops.py\u001b[0m in \u001b[0;36m_slice_helper\u001b[0;34m(tensor, slice_spec, var)\u001b[0m\n\u001b[1;32m    811\u001b[0m         \u001b[0mellipsis_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mellipsis_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[0mvar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 813\u001b[0;31m         name=name)\n\u001b[0m\u001b[1;32m    814\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    815\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/array_ops.py\u001b[0m in \u001b[0;36mstrided_slice\u001b[0;34m(input_, begin, end, strides, begin_mask, end_mask, ellipsis_mask, new_axis_mask, shrink_axis_mask, var, name)\u001b[0m\n\u001b[1;32m    977\u001b[0m       \u001b[0mellipsis_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mellipsis_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    978\u001b[0m       \u001b[0mnew_axis_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnew_axis_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 979\u001b[0;31m       shrink_axis_mask=shrink_axis_mask)\n\u001b[0m\u001b[1;32m    980\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m   \u001b[0mparent_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mstrided_slice\u001b[0;34m(input, begin, end, strides, begin_mask, end_mask, ellipsis_mask, new_axis_mask, shrink_axis_mask, name)\u001b[0m\n\u001b[1;32m  10392\u001b[0m                         \u001b[0mellipsis_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mellipsis_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  10393\u001b[0m                         \u001b[0mnew_axis_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnew_axis_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m> 10394\u001b[0;31m                         shrink_axis_mask=shrink_axis_mask, name=name)\n\u001b[0m\u001b[1;32m  10395\u001b[0m   \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  10396\u001b[0m   \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    791\u001b[0m         op = g.create_op(op_type_name, inputs, dtypes=None, name=scope,\n\u001b[1;32m    792\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 793\u001b[0;31m                          op_def=op_def)\n\u001b[0m\u001b[1;32m    794\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_stateful\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py\u001b[0m in \u001b[0;36mcreate_op\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    546\u001b[0m     return super(FuncGraph, self)._create_op_internal(  # pylint: disable=protected-access\n\u001b[1;32m    547\u001b[0m         \u001b[0mop_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 548\u001b[0;31m         compute_device)\n\u001b[0m\u001b[1;32m    549\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcapture\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_op_internal\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[1;32m   3427\u001b[0m           \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3428\u001b[0m           \u001b[0moriginal_op\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_default_original_op\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3429\u001b[0;31m           op_def=op_def)\n\u001b[0m\u001b[1;32m   3430\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_op_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3431\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\u001b[0m\n\u001b[1;32m   1771\u001b[0m           op_def, inputs, node_def.attr)\n\u001b[1;32m   1772\u001b[0m       self._c_op = _create_c_op(self._graph, node_def, grouped_inputs,\n\u001b[0;32m-> 1773\u001b[0;31m                                 control_input_ops)\n\u001b[0m\u001b[1;32m   1774\u001b[0m     \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[1;32m   1611\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1612\u001b[0m     \u001b[0;31m# Convert to ValueError for backwards compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1613\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1614\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1615\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mc_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Index out of range using input dim 2; input has only 2 dims for 'strided_slice_452' (op: 'StridedSlice') with input shapes: [?,?], [3], [3], [3] and with computed input tensors: input[3] = <1 1 1>."
     ]
    }
   ],
   "source": [
    "model = autoencoder(hidden_size, text_max_len, src_vocab, tar_vocab, embedding_dim, name='autoencoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected input_16 to have 2 dimensions, but got array with shape (108, 90, 150)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-258-7cc16b927c61>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m history = model.fit(x = [encoder_input_train, decoder_input_train], y = decoder_target_train, \\\n\u001b[1;32m      3\u001b[0m           \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mencoder_input_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_input_test\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_target_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m           batch_size = 4, callbacks=[es], epochs = 50)\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 728\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m           \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m           distribution_strategy=strategy)\n\u001b[0m\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m       \u001b[0mtotal_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_total_number_of_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data_adapter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36m_process_training_inputs\u001b[0;34m(model, x, y, batch_size, epochs, sample_weights, class_weights, steps_per_epoch, validation_split, validation_data, validation_steps, shuffle, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    545\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    548\u001b[0m     \u001b[0mval_adapter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36m_process_inputs\u001b[0;34m(model, x, y, batch_size, epochs, sample_weights, class_weights, shuffle, steps, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    592\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m         \u001b[0mcheck_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 594\u001b[0;31m         steps=steps)\n\u001b[0m\u001b[1;32m    595\u001b[0m   adapter = adapter_cls(\n\u001b[1;32m    596\u001b[0m       \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[1;32m   2470\u001b[0m           \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2471\u001b[0m           \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2472\u001b[0;31m           exception_prefix='input')\n\u001b[0m\u001b[1;32m   2473\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2474\u001b[0m     \u001b[0;31m# Get typespecs for the input data and sanitize it if necessary.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    563\u001b[0m                            \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m                            \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' dimensions, but got array '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 565\u001b[0;31m                            'with shape ' + str(data_shape))\n\u001b[0m\u001b[1;32m    566\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m           \u001b[0mdata_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected input_16 to have 2 dimensions, but got array with shape (108, 90, 150)"
     ]
    }
   ],
   "source": [
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience = 2)\n",
    "history = model.fit(x = [encoder_input_train, decoder_input_train], y = decoder_target_train, \\\n",
    "          validation_data = ([encoder_input_test, decoder_input_test], decoder_target_test),\n",
    "          batch_size = 4, callbacks=[es], epochs = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='test')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_index_to_word = src_tokenizer.index_word # 원문 단어 집합에서 정수 -> 단어를 얻음\n",
    "tar_word_to_index = tar_tokenizer.word_index # 요약 단어 집합에서 단어 -> 정수를 얻음\n",
    "tar_index_to_word = tar_tokenizer.index_word # 요약 단어 집합에서 정수 -> 단어를 얻음\n",
    "\n",
    "# 인코더 설계\n",
    "encoder_model = Model(inputs=encoder_inputs, outputs=[encoder_outputs, state_h, state_c])\n",
    "\n",
    "# 이전 시점의 상태들을 저장하는 텐서\n",
    "decoder_state_input_h = Input(shape=(hidden_size,))\n",
    "decoder_state_input_c = Input(shape=(hidden_size,))\n",
    "\n",
    "dec_emb2 = dec_emb_layer(decoder_inputs)\n",
    "# 문장의 다음 단어를 예측하기 위해서 초기 상태(initial_state)를 이전 시점의 상태로 사용. 이는 뒤의 함수 decode_sequence()에 구현\n",
    "# 훈련 과정에서와 달리 LSTM의 리턴하는 은닉 상태와 셀 상태인 state_h와 state_c를 버리지 않음.\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=[decoder_state_input_h, decoder_state_input_c])\n",
    "\n",
    "# 어텐션 함수\n",
    "decoder_hidden_state_input = Input(shape=(text_max_len, hidden_size))\n",
    "attn_out_inf, attn_states_inf = attn_layer([decoder_hidden_state_input, decoder_outputs2])\n",
    "decoder_inf_concat = Concatenate(axis=-1, name='concat')([decoder_outputs2, attn_out_inf])\n",
    "\n",
    "# 디코더의 출력층\n",
    "decoder_outputs2 = decoder_softmax_layer(decoder_inf_concat) \n",
    "\n",
    "# 최종 디코더 모델\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + [decoder_hidden_state_input,decoder_state_input_h, decoder_state_input_c],\n",
    "    [decoder_outputs2] + [state_h2, state_c2])\n",
    "\n",
    "def decode_sequence(input_seq):\n",
    "    # 입력으로부터 인코더의 상태를 얻음\n",
    "    e_out, e_h, e_c = encoder_model.predict(input_seq)\n",
    "\n",
    "     # <SOS>에 해당하는 토큰 생성\n",
    "    target_seq = np.zeros((1,1))\n",
    "    target_seq[0, 0] = tar_word_to_index['sostoken']\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition: # stop_condition이 True가 될 때까지 루프 반복\n",
    "\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + [e_out, e_h, e_c])\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_token = tar_index_to_word[sampled_token_index]\n",
    "\n",
    "        if(sampled_token!='eostoken'):\n",
    "            decoded_sentence += ' '+sampled_token\n",
    "\n",
    "        #  <eos>에 도달하거나 최대 길이를 넘으면 중단.\n",
    "        if (sampled_token == 'eostoken'  or len(decoded_sentence.split()) >= (text_max_len-1)):\n",
    "            stop_condition = True\n",
    "\n",
    "        # 길이가 1인 타겟 시퀀스를 업데이트\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # 상태를 업데이트 합니다.\n",
    "        e_h, e_c = h, c\n",
    "\n",
    "    return decoded_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq2text(input_seq):\n",
    "    temp=''\n",
    "    for i in input_seq:\n",
    "        if(i!=0):\n",
    "            temp = temp + src_index_to_word[i]+' '\n",
    "    return temp\n",
    "\n",
    "# 요약문의 정수 시퀀스를 텍스트 시퀀스로 변환\n",
    "def seq2summary(input_seq):\n",
    "    temp=''\n",
    "    for i in input_seq:\n",
    "        if((i!=0 and i!=tar_word_to_index['sostoken']) and i!=tar_word_to_index['eostoken']):\n",
    "            temp = temp + tar_index_to_word[i] + ' '\n",
    "    return temp\n",
    "\n",
    "\n",
    "for i in range(500, 1000):\n",
    "    print(\"원문 : \",seq2text(encoder_input_test[i]))\n",
    "    print(\"예측 :\",decode_sequence(encoder_input_test[i].reshape(1, text_max_len)))\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"novel_model1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "from attention import AttentionLayer\n",
    "model = load_model(\"novel_model1.h5\", custom_objects={'AttentionLayer':AttentionLayer})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
