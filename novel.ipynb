{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer \n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import urllib.request\n",
    "import tensorflow as tf\n",
    "np.random.seed(seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "os.environ[\"SM_FRAMEWORK\"] = \"tf.keras\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "books_path = \"./books-autoencoder/\"\n",
    "new_path = \"../\"\n",
    "books_list = sorted(os.listdir(books_path))\n",
    "\n",
    "documents = list()\n",
    "\n",
    "for i in books_list:\n",
    "    f = open(books_path + i, 'r')\n",
    "    doc = list(map(lambda x: x.replace(\"\\n\", \"\"), f.readlines()))\n",
    "    doc = [x.replace(\"</s>\", \"\") for x in doc]\n",
    "    documents.extend(doc)\n",
    "\n",
    "\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'그 연줄이 잠긴 자새와 연을 내게 쥐주고 집을 나설 때 섭섭해 울라카는 나를 보고 아부지는 노상 이런 말씀을 하셨 능기라 아부지가 보고 싶으모 이 연을 훨훨 띠아라 저 하늘 높이게 연이 나르는 곳이 바로 아부지가 기시는 곳이거덩 하고 말이다 나는 엄동 석달만이 아니고 봄가실에도 연을 날리며 연맨쿠로 멀리멀 리 떠 댕기는 아부지를 그리며 컸어 연이 작은 새가 돼서 아주 멀리 멀리로 날아가모 나도 연이 돼서 그렇게 하늘 꼭대리고 떠돌아댕겼제 내가 니 나이만 했을 때 바람 쌩쌩한 어느 겨울이었어 내가 날린 연 과 마실 아이의 연이 쌈을 붙잖았능가베 연줄이 서로 섞갈리자 나는 자새로 실이 다 풀리도록 연을 멀리로 띄아보냈거덩 낯짝만 하던 연 이 손가락맨큼 작아지고 마지막에는 바둑돌맨큼 작아져서 가물거릴 때까지 연줄을 죄 풀어주었제 둘러선 마실 아이들이 하늘 저 멀리로 콩알만해 진 연 두 개를 조마조마하게 쳐다 보았어'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_len = [len(s.split()) for s in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(text_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "텍스트의 최소 길이 : 2\n",
      "텍스트의 최대 길이 : 2406\n",
      "텍스트의 평균 길이 : 76.20321720921378\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('텍스트의 최소 길이 : {}'.format(np.min(text_len)))\n",
    "print('텍스트의 최대 길이 : {}'.format(np.max(text_len)))\n",
    "print('텍스트의 평균 길이 : {}'.format(np.mean(text_len)))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.boxplot(text_len)\n",
    "plt.title('Text')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.title('Text')\n",
    "plt.hist(text_len, bins=40)\n",
    "plt.xlabel('length of samples')\n",
    "plt.ylabel('number of samples')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_max_len = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def below_threshold_len(max_len, nested_list):\n",
    "    cnt = 0\n",
    "    for s in nested_list:\n",
    "        if(len(s.split()) <= max_len):\n",
    "            cnt = cnt + 1\n",
    "    print('전체 샘플 중 길이가 %s 이하인 샘플의 비율: %s'%(max_len, (cnt / len(nested_list))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플 중 길이가 300 이하인 샘플의 비율: 0.9872453835903293\n"
     ]
    }
   ],
   "source": [
    "below_threshold_len(text_max_len, documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input = [x for x in documents if len(x.split()) <= text_max_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_input = ['sostoken ' + x for x in encoder_input ]\n",
    "decoder_target = [x + ' eostoken' for x in encoder_input ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input = np.array(encoder_input)\n",
    "decoder_input = np.array(decoder_input)\n",
    "decoder_target = np.array(decoder_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6157 3673 3583 ... 3264 9845 2732]\n"
     ]
    }
   ],
   "source": [
    "indices = np.arange(encoder_input.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "print(indices)\n",
    "\n",
    "encoder_input = encoder_input[indices]\n",
    "decoder_input = decoder_input[indices]\n",
    "decoder_target = decoder_target[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "테스트 데이터의 수 : 2074\n"
     ]
    }
   ],
   "source": [
    "n_of_val = int(len(encoder_input)*0.2)\n",
    "print('테스트 데이터의 수 :',n_of_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_train = encoder_input[:-n_of_val]\n",
    "decoder_input_train = decoder_input[:-n_of_val]\n",
    "decoder_target_train = decoder_target[:-n_of_val]\n",
    "\n",
    "encoder_input_test = encoder_input[-n_of_val:]\n",
    "decoder_input_test = decoder_input[-n_of_val:]\n",
    "decoder_target_test = decoder_target[-n_of_val:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8298\n",
      "8298\n",
      "8298\n"
     ]
    }
   ],
   "source": [
    "print(len(encoder_input_train))\n",
    "print(len(decoder_input_train))\n",
    "print(len(decoder_target_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 데이터의 개수 : 8298\n",
      "훈련 레이블의 개수 : 8298\n",
      "테스트 데이터의 개수 : 2074\n",
      "테스트 레이블의 개수 : 2074\n"
     ]
    }
   ],
   "source": [
    "print('훈련 데이터의 개수 :', len(encoder_input_train))\n",
    "print('훈련 레이블의 개수 :',len(decoder_input_train))\n",
    "print('테스트 데이터의 개수 :',len(encoder_input_test))\n",
    "print('테스트 레이블의 개수 :',len(decoder_input_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_tokenizer = Tokenizer()\n",
    "src_tokenizer.fit_on_texts(encoder_input_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합(vocabulary)의 크기 : 125143\n",
      "등장 빈도가 6번 이하인 희귀 단어의 수: 115248\n",
      "단어 집합에서 희귀 단어를 제외시킬 경우의 단어 집합의 크기 9895\n",
      "단어 집합에서 희귀 단어의 비율: 92.09304555588406\n",
      "전체 등장 빈도에서 희귀 단어 등장 빈도 비율: 32.68718233340721\n"
     ]
    }
   ],
   "source": [
    "threshold = 7\n",
    "total_cnt = len(src_tokenizer.word_index) # 단어의 수\n",
    "rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n",
    "total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n",
    "rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\n",
    "\n",
    "# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\n",
    "for key, value in src_tokenizer.word_counts.items():\n",
    "    total_freq = total_freq + value\n",
    "\n",
    "    # 단어의 등장 빈도수가 threshold보다 작으면\n",
    "    if(value < threshold):\n",
    "        rare_cnt = rare_cnt + 1\n",
    "        rare_freq = rare_freq + value\n",
    "\n",
    "print('단어 집합(vocabulary)의 크기 :',total_cnt)\n",
    "print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))\n",
    "print('단어 집합에서 희귀 단어를 제외시킬 경우의 단어 집합의 크기 %s'%(total_cnt - rare_cnt))\n",
    "print(\"단어 집합에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\n",
    "print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_vocab = 110000\n",
    "src_tokenizer = Tokenizer(num_words = src_vocab) \n",
    "src_tokenizer.fit_on_texts(encoder_input_train)\n",
    "\n",
    "# 텍스트 시퀀스를 정수 시퀀스로 변환\n",
    "encoder_input_train = src_tokenizer.texts_to_sequences(encoder_input_train) \n",
    "encoder_input_test = src_tokenizer.texts_to_sequences(encoder_input_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tar_tokenizer = Tokenizer()\n",
    "tar_tokenizer.fit_on_texts(decoder_input_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합(vocabulary)의 크기 : 125144\n",
      "등장 빈도가 5번 이하인 희귀 단어의 수: 113493\n",
      "단어 집합에서 희귀 단어를 제외시킬 경우의 단어 집합의 크기 11651\n",
      "단어 집합에서 희귀 단어의 비율: 90.6899252061625\n",
      "전체 등장 빈도에서 희귀 단어 등장 빈도 비율: 30.258372293070153\n"
     ]
    }
   ],
   "source": [
    "threshold = 6\n",
    "total_cnt = len(tar_tokenizer.word_index) # 단어의 수\n",
    "rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n",
    "total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n",
    "rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\n",
    "\n",
    "# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\n",
    "for key, value in tar_tokenizer.word_counts.items():\n",
    "    total_freq = total_freq + value\n",
    "\n",
    "    # 단어의 등장 빈도수가 threshold보다 작으면\n",
    "    if(value < threshold):\n",
    "        rare_cnt = rare_cnt + 1\n",
    "        rare_freq = rare_freq + value\n",
    "\n",
    "print('단어 집합(vocabulary)의 크기 :',total_cnt)\n",
    "print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))\n",
    "print('단어 집합에서 희귀 단어를 제외시킬 경우의 단어 집합의 크기 %s'%(total_cnt - rare_cnt))\n",
    "print(\"단어 집합에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\n",
    "print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tar_vocab = 110000\n",
    "tar_tokenizer = Tokenizer(num_words = tar_vocab) \n",
    "tar_tokenizer.fit_on_texts(decoder_input_train)\n",
    "tar_tokenizer.fit_on_texts(decoder_target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_input_train = tar_tokenizer.texts_to_sequences(decoder_input_train) \n",
    "decoder_target_train = tar_tokenizer.texts_to_sequences(decoder_target_train)\n",
    "decoder_input_test = tar_tokenizer.texts_to_sequences(decoder_input_test)\n",
    "decoder_target_test = tar_tokenizer.texts_to_sequences(decoder_target_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_train = [index for index, sentence in enumerate(decoder_input_train) if len(sentence) == 1]\n",
    "drop_test = [index for index, sentence in enumerate(decoder_input_test) if len(sentence) == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "삭제할 훈련 데이터의 개수 : 0\n",
      "삭제할 테스트 데이터의 개수 : 0\n"
     ]
    }
   ],
   "source": [
    "print('삭제할 훈련 데이터의 개수 :',len(drop_train))\n",
    "print('삭제할 테스트 데이터의 개수 :',len(drop_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 데이터의 개수 : 8298\n",
      "훈련 레이블의 개수 : 8298\n",
      "테스트 데이터의 개수 : 2074\n",
      "테스트 레이블의 개수 : 2074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tako/anaconda3/lib/python3.7/site-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    }
   ],
   "source": [
    "encoder_input_train = np.delete(encoder_input_train, drop_train, axis=0)\n",
    "decoder_input_train = np.delete(decoder_input_train, drop_train, axis=0)\n",
    "decoder_target_train = np.delete(decoder_target_train, drop_train, axis=0)\n",
    "\n",
    "encoder_input_test = np.delete(encoder_input_test, drop_test, axis=0)\n",
    "decoder_input_test = np.delete(decoder_input_test, drop_test, axis=0)\n",
    "decoder_target_test = np.delete(decoder_target_test, drop_test, axis=0)\n",
    "\n",
    "print('훈련 데이터의 개수 :', len(encoder_input_train))\n",
    "print('훈련 레이블의 개수 :',len(decoder_input_train))\n",
    "print('테스트 데이터의 개수 :',len(encoder_input_test))\n",
    "print('테스트 레이블의 개수 :',len(decoder_input_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_train = pad_sequences(encoder_input_train, maxlen = text_max_len, padding='post')\n",
    "encoder_input_test = pad_sequences(encoder_input_test, maxlen = text_max_len, padding='post')\n",
    "decoder_input_train = pad_sequences(decoder_input_train, maxlen = text_max_len, padding='post')\n",
    "decoder_target_train = pad_sequences(decoder_target_train, maxlen = text_max_len, padding='post')\n",
    "decoder_input_test = pad_sequences(decoder_input_test, maxlen = text_max_len, padding='post')\n",
    "decoder_target_test = pad_sequences(decoder_target_test, maxlen = text_max_len, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 128\n",
    "hidden_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/thushv89/attention_keras/master/src/layers/attention.py\", filename=\"attention.py\")\n",
    "from attention import AttentionLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder(hidden_size, text_max_len, src_vocab, embedding_dim, name='encoder'):\n",
    "    \n",
    "    encoder_inputs = Input(shape=(text_max_len, ))\n",
    "\n",
    "    # 인코더의 임베딩 층\n",
    "    enc_emb = Embedding(src_vocab, embedding_dim)(encoder_inputs)\n",
    "\n",
    "    # 인코더의 LSTM 1\n",
    "    encoder_lstm1 = LSTM(hidden_size, return_sequences=True, return_state=True ,dropout = 0.4, recurrent_dropout = 0.4)\n",
    "    encoder_output1, state_h1, state_c1 = encoder_lstm1(enc_emb)\n",
    "\n",
    "    # 인코더의 LSTM 2\n",
    "    encoder_lstm2 = LSTM(hidden_size, return_sequences=True, return_state=True, dropout=0.4, recurrent_dropout=0.4)\n",
    "    encoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1)\n",
    "\n",
    "    # 인코더의 LSTM 3\n",
    "    encoder_lstm3 = LSTM(hidden_size, return_state=True, return_sequences=True, dropout=0.4, recurrent_dropout=0.4)\n",
    "    encoder_outputs, state_h, state_c= encoder_lstm3(encoder_output2)\n",
    "    \n",
    "    return Model(inputs = encoder_inputs, outputs = [encoder_outputs, state_h, state_c], name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder(tar_vocab, embedding_dim, hidden_size, name='decoder'):\n",
    "\n",
    "    encoder_outputs = Input(shape=(None, embedding_dim))\n",
    "    decoder_inputs = Input(shape=(None,))\n",
    "    state_h = Input(shape=(None, embedding_dim))\n",
    "    state_c = Input(shape=(None, embedding_dim))\n",
    "    \n",
    "    # 디코더의 임베딩 층\n",
    "    dec_emb_layer = Embedding(tar_vocab, embedding_dim)\n",
    "    dec_emb = dec_emb_layer(decoder_inputs)\n",
    "\n",
    "    # 디코더의 LSTM\n",
    "    decoder_lstm = LSTM(hidden_size, return_sequences = True, return_state = True, dropout = 0.4, recurrent_dropout=0.2)\n",
    "    decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state = [state_h, state_c])\n",
    "    \n",
    "    attn_layer = AttentionLayer(name='attention_layer')\n",
    "    attn_out, attn_states = attn_layer([encoder_outputs, decoder_outputs])\n",
    "\n",
    "    # 어텐션의 결과와 디코더의 hidden state들을 연결\n",
    "    decoder_concat_input = Concatenate(axis = -1, name='concat_layer')([decoder_outputs, attn_out])\n",
    "\n",
    "    # 디코더의 출력층\n",
    "    decoder_softmax_layer = Dense(tar_vocab, activation='softmax')\n",
    "    decoder_softmax_outputs = decoder_softmax_layer(decoder_concat_input) \n",
    "\n",
    "    return Model(inputs = [encoder_outputs, state_h, state_c, decoder_inputs], outputs = decoder_softmax_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autoencoder(hidden_size, text_max_len, src_vocab, tar_vocab, embedding_dim, name='autoencoder'):\n",
    "    encoder_inputs = Input(shape=(text_max_len, ))\n",
    "    decoder_inputs = Input(shape=(None,))\n",
    "\n",
    "    # 인코더의 임베딩 층\n",
    "    enc_emb = Embedding(src_vocab, embedding_dim)(encoder_inputs)\n",
    "\n",
    "    # 인코더의 LSTM 1\n",
    "    encoder_lstm1 = LSTM(hidden_size, return_sequences=True, return_state=True ,dropout = 0.4, recurrent_dropout = 0.4)\n",
    "    encoder_output1, state_h1, state_c1 = encoder_lstm1(enc_emb)\n",
    "\n",
    "    # 인코더의 LSTM 2\n",
    "    encoder_lstm2 = LSTM(hidden_size, return_sequences=True, return_state=True, dropout=0.4, recurrent_dropout=0.4)\n",
    "    encoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1)\n",
    "\n",
    "    # 인코더의 LSTM 3\n",
    "    encoder_lstm3 = LSTM(hidden_size, return_state=True, return_sequences=True, dropout=0.4, recurrent_dropout=0.4)\n",
    "    encoder_outputs, state_h, state_c= encoder_lstm3(encoder_output2)\n",
    "    \n",
    "    #return Model(inputs = encoder_inputs, outputs = [encoder_outputs, state_h, state_c], name=name)\n",
    "        \n",
    "    # 디코더의 임베딩 층\n",
    "    dec_emb_layer = Embedding(tar_vocab, embedding_dim)\n",
    "    dec_emb = dec_emb_layer(decoder_inputs)\n",
    "\n",
    "    # 디코더의 LSTM\n",
    "    decoder_lstm = LSTM(hidden_size, return_sequences = True, return_state = True, dropout = 0.4, recurrent_dropout=0.2)\n",
    "    decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state = [state_h, state_c])\n",
    "    \n",
    "    attn_layer = AttentionLayer(name='attention_layer')\n",
    "    attn_out, attn_states = attn_layer([encoder_outputs, decoder_outputs])\n",
    "\n",
    "    # 어텐션의 결과와 디코더의 hidden state들을 연결\n",
    "    decoder_concat_input = Concatenate(axis = -1, name='concat_layer')([decoder_outputs, attn_out])\n",
    "\n",
    "    # 디코더의 출력층\n",
    "    decoder_softmax_layer = Dense(tar_vocab, activation='softmax')\n",
    "    decoder_softmax_outputs = decoder_softmax_layer(decoder_concat_input) \n",
    "\n",
    "    #return Model(inputs = [encoder_outputs, state_h, state_c, decoder_inputs], outputs = decoder_softmax_outputs)\n",
    "    \n",
    "    return Model([encoder_inputs, decoder_inputs], outputs = decoder_softmax_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = autoencoder(hidden_size, text_max_len, src_vocab, tar_vocab, embedding_dim, name='autoencoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8298 samples, validate on 2074 samples\n",
      "Epoch 1/50\n",
      "  24/8298 [..............................] - ETA: 1:25:06 - loss: 10.0857WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-7cc16b927c61>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m history = model.fit(x = [encoder_input_train, decoder_input_train], y = decoder_target_train, \\\n\u001b[1;32m      3\u001b[0m           \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mencoder_input_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_input_test\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_target_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m           batch_size = 4, callbacks=[es], epochs = 50)\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 728\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m                 total_epochs=epochs)\n\u001b[0m\u001b[1;32m    325\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    121\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[1;32m    122\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[0;34m(input_fn)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[0;32m---> 86\u001b[0;31m                               distributed_function(input_fn))\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_counter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalled_without_tracing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    485\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 487\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    488\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1821\u001b[0m     \u001b[0;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1822\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1823\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1824\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1825\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1139\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1140\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1141\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1143\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1222\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m       flat_outputs = forward_function.call(\n\u001b[0;32m-> 1224\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1225\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1226\u001b[0m       \u001b[0mgradient_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delayed_rewrite_functions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    509\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    512\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience = 2)\n",
    "history = model.fit(x = [encoder_input_train, decoder_input_train], y = decoder_target_train, \\\n",
    "          validation_data = ([encoder_input_test, decoder_input_test], decoder_target_test),\n",
    "          batch_size = 4, callbacks=[es], epochs = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='test')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_index_to_word = src_tokenizer.index_word # 원문 단어 집합에서 정수 -> 단어를 얻음\n",
    "tar_word_to_index = tar_tokenizer.word_index # 요약 단어 집합에서 단어 -> 정수를 얻음\n",
    "tar_index_to_word = tar_tokenizer.index_word # 요약 단어 집합에서 정수 -> 단어를 얻음\n",
    "\n",
    "# 인코더 설계\n",
    "encoder_model = Model(inputs=encoder_inputs, outputs=[encoder_outputs, state_h, state_c])\n",
    "\n",
    "# 이전 시점의 상태들을 저장하는 텐서\n",
    "decoder_state_input_h = Input(shape=(hidden_size,))\n",
    "decoder_state_input_c = Input(shape=(hidden_size,))\n",
    "\n",
    "dec_emb2 = dec_emb_layer(decoder_inputs)\n",
    "# 문장의 다음 단어를 예측하기 위해서 초기 상태(initial_state)를 이전 시점의 상태로 사용. 이는 뒤의 함수 decode_sequence()에 구현\n",
    "# 훈련 과정에서와 달리 LSTM의 리턴하는 은닉 상태와 셀 상태인 state_h와 state_c를 버리지 않음.\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=[decoder_state_input_h, decoder_state_input_c])\n",
    "\n",
    "# 어텐션 함수\n",
    "decoder_hidden_state_input = Input(shape=(text_max_len, hidden_size))\n",
    "attn_out_inf, attn_states_inf = attn_layer([decoder_hidden_state_input, decoder_outputs2])\n",
    "decoder_inf_concat = Concatenate(axis=-1, name='concat')([decoder_outputs2, attn_out_inf])\n",
    "\n",
    "# 디코더의 출력층\n",
    "decoder_outputs2 = decoder_softmax_layer(decoder_inf_concat) \n",
    "\n",
    "# 최종 디코더 모델\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + [decoder_hidden_state_input,decoder_state_input_h, decoder_state_input_c],\n",
    "    [decoder_outputs2] + [state_h2, state_c2])\n",
    "\n",
    "def decode_sequence(input_seq):\n",
    "    # 입력으로부터 인코더의 상태를 얻음\n",
    "    e_out, e_h, e_c = encoder_model.predict(input_seq)\n",
    "\n",
    "     # <SOS>에 해당하는 토큰 생성\n",
    "    target_seq = np.zeros((1,1))\n",
    "    target_seq[0, 0] = tar_word_to_index['sostoken']\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition: # stop_condition이 True가 될 때까지 루프 반복\n",
    "\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + [e_out, e_h, e_c])\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_token = tar_index_to_word[sampled_token_index]\n",
    "\n",
    "        if(sampled_token!='eostoken'):\n",
    "            decoded_sentence += ' '+sampled_token\n",
    "\n",
    "        #  <eos>에 도달하거나 최대 길이를 넘으면 중단.\n",
    "        if (sampled_token == 'eostoken'  or len(decoded_sentence.split()) >= (text_max_len-1)):\n",
    "            stop_condition = True\n",
    "\n",
    "        # 길이가 1인 타겟 시퀀스를 업데이트\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # 상태를 업데이트 합니다.\n",
    "        e_h, e_c = h, c\n",
    "\n",
    "    return decoded_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq2text(input_seq):\n",
    "    temp=''\n",
    "    for i in input_seq:\n",
    "        if(i!=0):\n",
    "            temp = temp + src_index_to_word[i]+' '\n",
    "    return temp\n",
    "\n",
    "# 요약문의 정수 시퀀스를 텍스트 시퀀스로 변환\n",
    "def seq2summary(input_seq):\n",
    "    temp=''\n",
    "    for i in input_seq:\n",
    "        if((i!=0 and i!=tar_word_to_index['sostoken']) and i!=tar_word_to_index['eostoken']):\n",
    "            temp = temp + tar_index_to_word[i] + ' '\n",
    "    return temp\n",
    "\n",
    "\n",
    "for i in range(500, 1000):\n",
    "    print(\"원문 : \",seq2text(encoder_input_test[i]))\n",
    "    print(\"예측 :\",decode_sequence(encoder_input_test[i].reshape(1, text_max_len)))\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"novel_model1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "from attention import AttentionLayer\n",
    "model = load_model(\"novel_model1.h5\", custom_objects={'AttentionLayer':AttentionLayer})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
