{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a6a12e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from numpy.linalg import norm\n",
    "from collections import namedtuple\n",
    "import gensim\n",
    "import json\n",
    "import re\n",
    "from textsplit.tools import get_penalty, get_segments\n",
    "from textsplit.algorithm import split_optimal, get_total, get_gains\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "import pickle\n",
    "\n",
    "path = \"./txt/\"\n",
    "seg_path = \"./segmented/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "fee911ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(input_path):\n",
    "    wlem = nltk.WordNetLemmatizer()\n",
    "    stopwords_list = stopwords.words('english') #nltk에서 제공하는 불용어사전 이용\n",
    "    \n",
    "    f = open(path + input_path, 'r')\n",
    "    texts = f.read().replace(\"\\n\", \"\")\n",
    "    texts = texts.replace(\"   \", \" \")\n",
    "    text = texts.replace(\"  \", \" \")\n",
    "\n",
    "\n",
    "    # 문장별로 잘려서 들어가있는 본문\n",
    "    splitted_text = list()\n",
    "    original_text = list()\n",
    "\n",
    "    text = re.sub(\"[^ ㄱ-ㅣ가-힣0-9a-zA-Z\\.|\\?|\\!|\\n]+\", \"\", text).lower()\n",
    "    sents = re.split(r\"[\\?|\\.|\\!|\\n]\", text)\n",
    "\n",
    "    for i in range(len(sents)):\n",
    "        \"\"\"\n",
    "        토크나이징을 통한 불용어 제거 및 어근만 남기기\n",
    "        \"\"\"\n",
    "        word_tokens = nltk.word_tokenize(sents[i])\n",
    "        tokens_pos = nltk.pos_tag(word_tokens)\n",
    "        \n",
    "        words = []\n",
    "        for word, pos in tokens_pos:\n",
    "            words.append(word)\n",
    "\n",
    "        # lemma 사용하기\n",
    "        lemmatized_words = []\n",
    "        for word in words:\n",
    "            new_word = wlem.lemmatize(word)\n",
    "            lemmatized_words.append(new_word)\n",
    "\n",
    "            \n",
    "        #print('stopwords: ', stopwords_list)\n",
    "        unique_NN_words = set(lemmatized_words)\n",
    "        final_NN_words = lemmatized_words\n",
    "\n",
    "        # 불용어 제거\n",
    "        for word in unique_NN_words:\n",
    "            if word in stopwords_list:\n",
    "                while word in final_NN_words: final_NN_words.remove(word)\n",
    "\n",
    "        splitted_text.append(\" \".join(final_NN_words) + \"</s>\" )\n",
    "        original_text.append(sents[i] + \"</s>\")\n",
    "\n",
    "    return splitted_text, original_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a9c4f37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vector(wrdvec_path):\n",
    "    model = gensim.models.KeyedVectors.load_word2vec_format(wrdvec_path, binary=True, unicode_errors='ignore')\n",
    "    wrdvecs = pd.DataFrame(model.vectors, index=model.vocab)\n",
    "\n",
    "    return wrdvecs\n",
    "\n",
    "\n",
    "def segmentation(wrdvecs, sentenced_text):\n",
    "    # sklearn CountVectorizer 클래스: 문서를 token count matrix로 변환하는 클래스\n",
    "    vecr = CountVectorizer(vocabulary=wrdvecs.index)\n",
    "\n",
    "    sentence_vectors = vecr.transform(sentenced_text).dot(wrdvecs)\n",
    "\n",
    "    print(len(sentenced_text))\n",
    "    segment_len = len(sentenced_text) // 10  # segment target length in sentences\n",
    "\n",
    "    print(\"segment_len: \", segment_len)\n",
    "    try:\n",
    "        penalty = get_penalty([sentence_vectors], segment_len)\n",
    "    except ZeroDivisionError:\n",
    "        return 0\n",
    "    try:\n",
    "        optimal_segmentation = split_optimal(sentence_vectors, penalty, seg_limit=segment_len)\n",
    "    except AssertionError:\n",
    "        return 0\n",
    "\n",
    "    segmented_text = get_segments(sentenced_text, optimal_segmentation)\n",
    "\n",
    "    return segmented_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab983398",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "wrdvec_path = './ko/ko.bin'\n",
    "wrdvecs = get_vector(wrdvec_path)\n",
    "\n",
    "file_list = os.listdir(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fec89767",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'all'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to /Users/hbae/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
      "[nltk_data]    | Downloading package alpino to\n",
      "[nltk_data]    |     /Users/hbae/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
      "[nltk_data]    | Downloading package biocreative_ppi to\n",
      "[nltk_data]    |     /Users/hbae/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
      "[nltk_data]    | Downloading package brown to /Users/hbae/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
      "[nltk_data]    | Downloading package brown_tei to\n",
      "[nltk_data]    |     /Users/hbae/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
      "[nltk_data]    | Downloading package cess_cat to\n",
      "[nltk_data]    |     /Users/hbae/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
      "[nltk_data]    | Downloading package cess_esp to\n",
      "[nltk_data]    |     /Users/hbae/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     /Users/hbae/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     /Users/hbae/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /Users/hbae/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
      "[nltk_data]    | Downloading package comparative_sentences to\n",
      "[nltk_data]    |     /Users/hbae/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
      "[nltk_data]    | Downloading package comtrans to\n",
      "[nltk_data]    |     /Users/hbae/nltk_data...\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     /Users/hbae/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     /Users/hbae/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
      "[nltk_data]    | Downloading package conll2007 to\n",
      "[nltk_data]    |     /Users/hbae/nltk_data...\n",
      "[nltk_data]    | Downloading package crubadan to\n",
      "[nltk_data]    |     /Users/hbae/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     /Users/hbae/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
      "[nltk_data]    | Downloading package dolch to /Users/hbae/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
      "[nltk_data]    | Downloading package europarl_raw to\n",
      "[nltk_data]    |     /Users/hbae/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
      "[nltk_data]    | Downloading package floresta to\n",
      "[nltk_data]    |     /Users/hbae/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
      "[nltk_data]    | Downloading package framenet_v15 to\n",
      "[nltk_data]    |     /Users/hbae/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
      "[nltk_data]    | Downloading package framenet_v17 to\n",
      "[nltk_data]    |     /Users/hbae/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     /Users/hbae/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /Users/hbae/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /Users/hbae/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
      "[nltk_data]    | Downloading package ieer to /Users/hbae/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /Users/hbae/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
      "[nltk_data]    | Downloading package indian to\n",
      "[nltk_data]    |     /Users/hbae/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
      "[nltk_data]    | Downloading package jeita to /Users/hbae/nltk_data...\n",
      "[nltk_data]    | Downloading package kimmo to /Users/hbae/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
      "[nltk_data]    | Downloading package knbc to /Users/hbae/nltk_data...\n",
      "[nltk_data]    | Downloading package lin_thesaurus to\n",
      "[nltk_data]    |     /Users/hbae/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
      "[nltk_data]    | Downloading package mac_morpho to\n",
      "[nltk_data]    |     /Users/hbae/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
      "[nltk_data]    | Downloading package machado to\n",
      "[nltk_data]    |     /Users/hbae/nltk_data...\n",
      "[nltk_data]    | Downloading package masc_tagged to\n",
      "[nltk_data]    |     /Users/hbae/nltk_data...\n",
      "[nltk_data]    | Downloading package moses_sample to\n",
      "[nltk_data]    |     /Users/hbae/nltk_data...\n",
      "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /Users/hbae/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
      "[nltk_data]    | Downloading package names to /Users/hbae/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/names.zip.\n",
      "[nltk_data]    | Downloading package nombank.1.0 to\n",
      "[nltk_data]    |     /Users/hbae/nltk_data...\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     /Users/hbae/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
      "[nltk_data]    | Downloading package omw to /Users/hbae/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/omw.zip.\n",
      "[nltk_data]    | Downloading package opinion_lexicon to\n",
      "[nltk_data]    |     /Users/hbae/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
      "[nltk_data]    | Downloading package paradigms to\n",
      "[nltk_data]    |     /Users/hbae/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
      "[nltk_data]    | Downloading package pil to /Users/hbae/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
      "[nltk_data]    | Downloading package pl196x to\n",
      "[nltk_data]    |     /Users/hbae/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     /Users/hbae/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
      "[nltk_data]    | Downloading package problem_reports to\n",
      "[nltk_data]    |     /Users/hbae/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
      "[nltk_data]    | Downloading package propbank to\n",
      "[nltk_data]    |     /Users/hbae/nltk_data...\n",
      "[nltk_data]    | Downloading package ptb to /Users/hbae/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
      "[nltk_data]    | Downloading package product_reviews_1 to\n",
      "[nltk_data]    |     /Users/hbae/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
      "[nltk_data]    | Downloading package product_reviews_2 to\n",
      "[nltk_data]    |     /Users/hbae/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
      "[nltk_data]    | Downloading package pros_cons to\n",
      "[nltk_data]    |     /Users/hbae/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
      "[nltk_data]    | Downloading package qc to /Users/hbae/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     /Users/hbae/nltk_data...\n",
      "[nltk_data]    | Downloading package rte to /Users/hbae/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
      "[nltk_data]    | Downloading package semcor to\n",
      "[nltk_data]    |     /Users/hbae/nltk_data...\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     /Users/hbae/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
      "[nltk_data]    | Downloading package sentiwordnet to\n",
      "[nltk_data]    |     /Users/hbae/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
      "[nltk_data]    | Downloading package sentence_polarity to\n",
      "[nltk_data]    |     /Users/hbae/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     /Users/hbae/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
      "[nltk_data]    | Downloading package sinica_treebank to\n",
      "[nltk_data]    |     /Users/hbae/nltk_data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
      "[nltk_data]    | Downloading package smultron to\n",
      "[nltk_data]    |     /Users/hbae/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     /Users/hbae/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /Users/hbae/nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package subjectivity to\n",
      "[nltk_data]    |     /Users/hbae/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     /Users/hbae/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
      "[nltk_data]    | Downloading package switchboard to\n",
      "[nltk_data]    |     /Users/hbae/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
      "[nltk_data]    | Downloading package timit to /Users/hbae/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     /Users/hbae/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /Users/hbae/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     /Users/hbae/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
      "[nltk_data]    | Downloading package udhr to /Users/hbae/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
      "[nltk_data]    | Downloading package udhr2 to /Users/hbae/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     /Users/hbae/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
      "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
      "[nltk_data]    |     /Users/hbae/nltk_data...\n",
      "[nltk_data]    | Downloading package verbnet to\n",
      "[nltk_data]    |     /Users/hbae/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
      "[nltk_data]    | Downloading package verbnet3 to\n",
      "[nltk_data]    |     /Users/hbae/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     /Users/hbae/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /Users/hbae/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/wordnet.zip.\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /Users/hbae/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
      "[nltk_data]    | Downloading package words to /Users/hbae/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/words.zip.\n",
      "[nltk_data]    | Downloading package ycoe to /Users/hbae/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
      "[nltk_data]    | Downloading package rslp to /Users/hbae/nltk_data...\n",
      "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     /Users/hbae/nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     /Users/hbae/nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /Users/hbae/nltk_data...\n",
      "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
      "[nltk_data]    | Downloading package punkt to /Users/hbae/nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     /Users/hbae/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
      "[nltk_data]    | Downloading package sample_grammars to\n",
      "[nltk_data]    |     /Users/hbae/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
      "[nltk_data]    | Downloading package spanish_grammars to\n",
      "[nltk_data]    |     /Users/hbae/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
      "[nltk_data]    | Downloading package basque_grammars to\n",
      "[nltk_data]    |     /Users/hbae/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
      "[nltk_data]    | Downloading package large_grammars to\n",
      "[nltk_data]    |     /Users/hbae/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     /Users/hbae/nltk_data...\n",
      "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     /Users/hbae/nltk_data...\n",
      "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
      "[nltk_data]    |     /Users/hbae/nltk_data...\n",
      "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
      "[nltk_data]    | Downloading package word2vec_sample to\n",
      "[nltk_data]    |     /Users/hbae/nltk_data...\n",
      "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     /Users/hbae/nltk_data...\n",
      "[nltk_data]    | Downloading package mte_teip5 to\n",
      "[nltk_data]    |     /Users/hbae/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /Users/hbae/nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
      "[nltk_data]    |     /Users/hbae/nltk_data...\n",
      "[nltk_data]    |   Unzipping\n",
      "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
      "[nltk_data]    | Downloading package perluniprops to\n",
      "[nltk_data]    |     /Users/hbae/nltk_data...\n",
      "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
      "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]    |     /Users/hbae/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
      "[nltk_data]    | Downloading package vader_lexicon to\n",
      "[nltk_data]    |     /Users/hbae/nltk_data...\n",
      "[nltk_data]    | Downloading package porter_test to\n",
      "[nltk_data]    |     /Users/hbae/nltk_data...\n",
      "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
      "[nltk_data]    | Downloading package wmt15_eval to\n",
      "[nltk_data]    |     /Users/hbae/nltk_data...\n",
      "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
      "[nltk_data]    | Downloading package mwa_ppdb to\n",
      "[nltk_data]    |     /Users/hbae/nltk_data...\n",
      "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection all\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "03cc3e9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1567\n",
      "segment_len:  156\n",
      "157\n",
      "segment_len:  15\n",
      "4982\n",
      "segment_len:  498\n",
      "3046\n",
      "segment_len:  304\n",
      "1108\n",
      "segment_len:  110\n",
      "4157\n",
      "segment_len:  415\n",
      "276\n",
      "segment_len:  27\n",
      "376\n",
      "segment_len:  37\n",
      "5932\n",
      "segment_len:  593\n",
      "1113\n",
      "segment_len:  111\n",
      "1638\n",
      "segment_len:  163\n",
      "20\n",
      "segment_len:  2\n",
      "7036\n",
      "segment_len:  703\n",
      "2905\n",
      "segment_len:  290\n",
      "4709\n",
      "segment_len:  470\n",
      "175\n",
      "segment_len:  17\n",
      "655\n",
      "segment_len:  65\n",
      "7689\n",
      "segment_len:  768\n",
      "451\n",
      "segment_len:  45\n",
      "2145\n",
      "segment_len:  214\n",
      "7896\n",
      "segment_len:  789\n",
      "512\n",
      "segment_len:  51\n",
      "166\n",
      "segment_len:  16\n",
      "4589\n",
      "segment_len:  458\n",
      "3631\n",
      "segment_len:  363\n",
      "3851\n",
      "segment_len:  385\n",
      "4596\n",
      "segment_len:  459\n",
      "7142\n",
      "segment_len:  714\n",
      "2881\n",
      "segment_len:  288\n",
      "3703\n",
      "segment_len:  370\n",
      "2039\n",
      "segment_len:  203\n",
      "7817\n",
      "segment_len:  781\n",
      "4763\n",
      "segment_len:  476\n",
      "2030\n",
      "segment_len:  203\n",
      "9023\n",
      "segment_len:  902\n",
      "2319\n",
      "segment_len:  231\n",
      "115\n",
      "segment_len:  11\n",
      "2901\n",
      "segment_len:  290\n",
      "5502\n",
      "segment_len:  550\n",
      "1630\n",
      "segment_len:  163\n",
      "5603\n",
      "segment_len:  560\n",
      "4569\n",
      "segment_len:  456\n",
      "1157\n",
      "segment_len:  115\n",
      "968\n",
      "segment_len:  96\n",
      "669\n",
      "segment_len:  66\n",
      "5801\n",
      "segment_len:  580\n",
      "13222\n",
      "segment_len:  1322\n",
      "6237\n",
      "segment_len:  623\n",
      "3003\n",
      "segment_len:  300\n",
      "9109\n",
      "segment_len:  910\n",
      "119\n",
      "segment_len:  11\n",
      "846\n",
      "segment_len:  84\n",
      "3646\n",
      "segment_len:  364\n",
      "953\n",
      "segment_len:  95\n",
      "7029\n",
      "segment_len:  702\n",
      "3398\n",
      "segment_len:  339\n",
      "3583\n",
      "segment_len:  358\n",
      "4316\n",
      "segment_len:  431\n",
      "6037\n",
      "segment_len:  603\n",
      "4884\n",
      "segment_len:  488\n",
      "3050\n",
      "segment_len:  305\n",
      "14740\n",
      "segment_len:  1474\n",
      "3986\n",
      "segment_len:  398\n",
      "1331\n",
      "segment_len:  133\n",
      "7048\n",
      "segment_len:  704\n",
      "3227\n",
      "segment_len:  322\n",
      "5805\n",
      "segment_len:  580\n",
      "8274\n",
      "segment_len:  827\n",
      "7809\n",
      "segment_len:  780\n",
      "8397\n",
      "segment_len:  839\n",
      "1865\n",
      "segment_len:  186\n",
      "5754\n",
      "segment_len:  575\n",
      "3829\n",
      "segment_len:  382\n",
      "8599\n",
      "segment_len:  859\n",
      "2039\n",
      "segment_len:  203\n",
      "5593\n",
      "segment_len:  559\n",
      "1950\n",
      "segment_len:  195\n",
      "4421\n",
      "segment_len:  442\n",
      "2364\n",
      "segment_len:  236\n",
      "965\n",
      "segment_len:  96\n",
      "20611\n",
      "segment_len:  2061\n",
      "2812\n",
      "segment_len:  281\n",
      "4020\n",
      "segment_len:  402\n",
      "328\n",
      "segment_len:  32\n",
      "87\n",
      "segment_len:  8\n",
      "4034\n",
      "segment_len:  403\n",
      "5515\n",
      "segment_len:  551\n",
      "1754\n",
      "segment_len:  175\n",
      "10869\n",
      "segment_len:  1086\n",
      "327\n",
      "segment_len:  32\n",
      "468\n",
      "segment_len:  46\n",
      "1715\n",
      "segment_len:  171\n",
      "7385\n",
      "segment_len:  738\n",
      "168\n",
      "segment_len:  16\n",
      "536\n",
      "segment_len:  53\n",
      "616\n",
      "segment_len:  61\n",
      "996\n",
      "segment_len:  99\n",
      "5291\n",
      "segment_len:  529\n",
      "549\n",
      "segment_len:  54\n",
      "5908\n",
      "segment_len:  590\n"
     ]
    }
   ],
   "source": [
    "for i in file_list[:100]:\n",
    "    title = i[:-5]\n",
    "    sentenced_text, original_text = preprocessing(i)\n",
    "\n",
    "    segmented_text = segmentation(wrdvecs, sentenced_text)\n",
    "    index = [len(segment) for segment in segmented_text]\n",
    "    \n",
    "    final_segment = list()\n",
    "    for i in index:\n",
    "        final_segment.append(original_text[:i])\n",
    "        original_text = original_text[i:]\n",
    "\n",
    "    with open(seg_path + 'seg+' + title + '.txt', 'wt') as f:\n",
    "        for i, segment_sentences in enumerate(segmented_text):\n",
    "            segment_str = ' // '.join(segment_sentences)\n",
    "            print(segment_str + '\\n<' + \"=\" * 30, file=f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "e5151b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "new_path = \"./eng_segment/\"\n",
    "file_list = os.listdir(seg_path)\n",
    "\n",
    "\n",
    "index = 0\n",
    "documents = dict()\n",
    "for i in file_list:\n",
    "    seg = list()\n",
    "    f = open(seg_path + i, 'r')\n",
    "    line = f.readlines()\n",
    "    title = i[4:-9]\n",
    "\n",
    "    for j in range(len(line)):\n",
    "        if \" // \" in line[j]:\n",
    "            segments = line[j].split(\" // \")\n",
    "            para = \" \".join(segments)\n",
    "            seg.append(para)\n",
    "    \n",
    "    full_text = \"\".join(seg)\n",
    "    \n",
    "    with open(new_path + title + '.txt', 'w', encoding='utf-8') as newf:\n",
    "        newf.write(full_text)\n",
    "\n",
    "            \n",
    "    f.close()\n",
    "    newf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36dc1a9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
